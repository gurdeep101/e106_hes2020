---
title: "P1_practice_mid_term_hes_e106"
author: "Gurdeep"
date: "21/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
rm(list = ls())

setwd("/cloud/project/practice_mid_term") # change to local directory before running

library(knitr)
library(onewaytests)
library(olsrr)
library(lmtest)
library(MASS)
library(ALSM)

pmq1 <- read.csv('PM Q1.csv')
head(pmq1)
colnames(pmq1)
dim(pmq1)

```

1a. a- ) Build a regression model to predict Y as a function of X. Write down the regression model, Is the regression model significant? (5 points)

```{r}
reg1a <- lm(Y~X, data = pmq1)
summary(reg1a)
ei_p1 <- reg1a$residuals
```

The regression model is Y = -81432.946 + 158.950 * X

The model has F-statistic of 1063 with p-value < 2.2e-16. Individual coefficients also have small p-values. This leads us to interpret that the model is significant.

1b. b-) Check all the assumptions related to the regression model and perform Brown-Forsythe Test. (10 points)

```{r}
# plot residuals and other standard plots
old.par <- par(mfrow = c(2,2))
plot(reg1a)
old.par

newer.par = par(mfrow = c(2,2))
plot(pmq1$X, pmq1$Y, xlab = 'X', ylab = 'Y')
plot(pmq1$X,ei_p1, xlab = 'X', ylab = 'Residuals')
boxplot(ei_p1, horizontal = TRUE, staplewax = 0.5, col = 3, xlab = 'Residual')
hist(ei_p1, main = 'Histogram of Residuals')
newer.par

# BF test
bf.data <- data.frame(cbind(pmq1, reg1a$residuals, reg1a$fitted.values))
colnames(bf.data)
dimnames(bf.data)[[2]][3:4] <- c('residuals', 'fitted.values')
colnames(bf.data)

bf.data1 <- data.frame(cbind(bf.data, ind = as.factor(I(bf.data$X<=median(bf.data$X))*1)))
dim(bf.data1) # Indicator column added at end

bf.test(residuals~ind, data = bf.data1)
```

From the residuals vs fitted plot we see that residuals diverge as X value increases. This pattern is also seen in the plot of X vs Y and Residuals vs X.
From Q-Q plot we see that the actual vs expected frequencies line is not 45 degrees and that the residuals plot diverges after 1.5 SD
There is presence of outliers
Boxplot is not symmetrical with many outliers.
Histogram of residuals has a long right tail.

Hence we conclude that the following assumptions are not satisfied:

Linearity of the regression function
Error terms have constant variance
Error terms are independent
Error terms are normally distributed
Model fits all but 1 or few outliers.

1c-)Do we need to transform Y? Use Box-Cox procedure to find out appropriate transformation of Y and justify your choice. (10 points)

From the scatterplot of X and Y we see that the Y values for higher values of X are dispersed. This indicates that some transformation is needed. We explore with BoxCox below

```{r}
boxcox(reg1a, lambda = seq(-1.0, 1.0,0.1))

```

The results are in the middle of lambda = 0 implying log (Y) and lambda = -0.5 implying 1/sqrt(Y). We explore both transformation. For transformation of X we try log transform to allow for dispersion.

```{r}
# explore log transform of both X & Y
reg1c_1 <- lm(log(Y)~log(X), data = pmq1)
summary(reg1c_1)
ei_1c_1 <- reg1c_1$residuals

# plot residuals and other standard plots
old.par <- par(mfrow = c(2,2))
plot(reg1c_1)
old.par

newer.par = par(mfrow = c(2,2))
plot(log(pmq1$X), log(pmq1$Y), xlab = 'Log X', ylab = 'log_Y')
plot(log(pmq1$X),ei_1c_1, xlab = 'log X', ylab = 'Residuals')
boxplot(ei_1c_1, horizontal = TRUE, staplewax = 0.5, col = 3, xlab = 'Residual')
hist(ei_1c_1, main = 'Histogram of Residuals')
newer.par

# explore transformation 1/sqrt(Y) and log(X)
reg_1c_2 <- lm(1/sqrt(Y)~log(Y), data = pmq1)
summary(reg_1c_2)
ei_1c_2 <- reg_1c_2$residuals

# plot residuals and other standard plots
old.par <- par(mfrow = c(2,2))
plot(reg_1c_2)
old.par

newer.par = par(mfrow = c(2,2))
plot(log(pmq1$X), 1/sqrt(pmq1$Y), xlab = 'Log X', ylab = '1/sqrt_Y')
plot(log(pmq1$X),ei_1c_2, xlab = 'log X', ylab = 'Residuals')
boxplot(ei_1c_2, horizontal = TRUE, staplewax = 0.5, col = 3, xlab = 'Residual')
hist(ei_1c_2, main = 'Histogram of Residuals')
newer.par
```

From the plots above we see that the log transform of both variables is the final model.The 2nd transform of 1/sqrt(Y) has very high R2 which itself may be a problem, and the residual plots are also not in line. 

1d-) Use the final model to predict Y for three new X values, 1100, 3000 and 4900. which methods would use you to calculate the joint 90% confidence intervals? Justify your choice and calculate the confidence interval using the final model in part c.  (15 points)


We explore both bonferroni and working-hotelling methods for calculation of 90% jt CI
```{r}
xh <- c(1100, 3000, 4900)
# make prediction
pred <- predict.lm(reg1c_1, data.frame(X = xh), level = 0.90, se.fit = TRUE)
pred 

# bonferroni method
b <- rep(qt(1-0.90/(2*length(xh)), nrow(pmq1)-2), length(xh))
b

bonf_final <- rbind(exp(pred$fit) - b*(exp(pred$se.fit)), exp(pred$fit) + b*exp(pred$se.fit))
bonf_final 

# w-h method
w <- rep(sqrt(2*qf(0.90, 2, nrow(pmq1)-2)), length(xh)) # get f-value and repeat 3 times
wh_final <- rbind(exp(pred$fit) - w*exp(pred$se.fit), exp(pred$fit) + w*exp(pred$se.fit)) # yhat + & - (W * SE of prediction)
wh_final

# compare both methods
bonf_final[2,1:3] - bonf_final[1,1:3]

wh_final[2,1:3] - wh_final[1,1:3]
```

Since the bonferroni interval is narrower than the WH interval we use the bonferroni interval as the final interval.


---
title: "Homework 1 Solutions"
author: "Gurdeep Singh"
date: "9/14/2020"
output:
  html_document: default
  pdf_document: default


## Problem 1

	Refer to the Grade point average Data. The director of admissions of a small college selected 120 students at random from the new freshman class in a study to determine whether a student's grade point average (GPA) at the end of the freshman year (Y) can be predicted from the ACT test score (X). (30 points)

a-)Obtain the least squares estimates of $\beta_{0}$ and $\beta_{1}$, and state the estimated regression function. (5pts)\
b-)	Plot the estimated regression function and the data. "Does the estimated regression function appear to fit the data well? (5pts)\
c-)Obtain a point estimate of the mean freshman GPA for students with ACT test score X = 30. (5pts)\
d-)What is the point estimate of the change in the mean response when the entrance test score increases by one point? (5pts)\
e-)Obtain the residuals $\epsilon_{i}$. Do they sum to zero? (5pts)\
f-)Estimate $\sigma$^2 and $\sigma$. In what units is σ expressed? (5pts)\

# Solutions:
a-)
Solution:
```{r}
library(knitr)

# read in data
gpa <- read.csv("/cloud/project/HW1/Grade Point Average Data.csv")
colnames(gpa) = c('student_gpa', 'act_score')
head(gpa)

# fit regression, obtain coefficients and residuals
reg_q1 <- lm(student_gpa~act_score, data = gpa)
print(reg_q1$coefficients)
intercept_q1 = reg_q1$coefficients[1]
act_score_q1 = reg_q1$coefficients[2]
res_q1 = reg_q1$residuals
```
As seen above, the coefficients are $\beta_0$ = 2.11404929 and $\beta_1$ = 0.03882713

b) 
Solution: 
```{r}
library(knitr)

plot(gpa$act_score, gpa$student_gpa, xlab = "ACT Test Score", ylab = "GPA")
abline(reg_q1, col = 'red')
```

As seen above the points are widely distributed and not clustered around the line. Hence the estimated regression function is not a good fit for the data provided.

c) 
Solution: 
```{r}
act_score = 30
mean_freshman_gpa = intercept_q1 + (act_score * act_score_q1)
mean_freshman_gpa
```
Point estimate of the mean freshman GPA for students with score 30 is 3.278863

d) 
Solution: 
```{r}
print(act_score_q1)
```
For a 1 point increase in entrance test score the GPA increases by the $beta_1$ coefficient = 0.0388 points

e) 
Solution: 
```{r}
res_q1[1:10] # residuals were calculated in 1a; printing 1st 10 variables here
ssr_q1 = sum(res_q1)
print(ssr_q1)
```
The residuals are not exactly zero but a very small value as seen above and can be considered as summing to zero for all practical purposes.

f) 
Solution: 
```{r}
ei2 = sum((res_q1)^2)
sigma2 = ei2/(length(res_q1)-2)
sigma = sqrt(sigma2)

print(sigma2)
print(sigma)

```
$\sigma$ is expressed in units of GPA

## Problem 2

Typographical errors shown below are the number of galleys for a manuscript (X) and the dollar cost of correcting typographical errors (Y) in a random sample of recent orders handled by a firm specializing in technical manuscripts. Assume that the regression model Yi = β1X1 + ε_i  is appropriate, with normally distributed independent error terms whose variance is a σ^2 = 16. (20 pts)

a) Evaluate the likelihood function for $\beta_{1}= 1,2, 3,…,100$. For which of $\beta_{1}$ values is the likelihood function largest? (10pts)\

b) The maximum likelihood estimator is $b_{1}=\sum X_{i} Y_{i}/\sum X_{i}^2$.  Find the maximum likelihood estimate. Are your results in part (a) consistent with this estimate? (10 pts)\

## Solutions:
a) 
```{r}
# input data
x_num_galley <- c(7, 12, 4, 14, 25, 30)
y_corr_cost <- c(128, 213, 75, 250, 446, 540)
n_beta <- 100
sig2_err <- 16 # variance given


# empty vectors to store output; len = n_beta to store all likelihood values
lkhood_val <- vector(mode = 'integer', length = n_beta) 

for (beta_val in 1: n_beta) {
  #print(beta_val)
  interim1 <-  beta_val * x_num_galley
  interim2 <-  (y_corr_cost - interim1)^2
  interim3 <-  sum(interim2)
  interim4 <- exp(interim3/(-2*sig2_err))
  interim5 <- interim4/((2*pi*sig2_err)^(length(x_num_galley)/2))
  lkhood_val[beta_val] <- interim5
}

# get index of value which has maximum value of likelihood
# max index value corresponds to beta
which.max(lkhood_val)

```
Value of likelihood function is largest for $\beta_1$ = 18

b) 

```{r}
mle_2b <- sum(x_num_galley*y_corr_cost)/sum(x_num_galley * x_num_galley)
mle_2b
```

As seen above the results in part 2a and part 2b are consistent.

## Problem 3

Refer to the CDI data set. The number of active physicians in a CDI (Y) is
expected to be related to total population, number of hospital beds, and total personal income. (30 points)

a)	Regress the number of active physicians in turn on each of the three predictor variables. State the estimated regression functions. (10 points)\
b)	Plot the three estimated regression functions and data on separate graphs. Does a linear regression relation appear to provide a good fit for each of the three predictor variables? (10 points)\
c)	Calculate MSE for each of the three predictor variables. Which predictor variable leads to the smallest variability around the fitted regression line? Which variable would you use the estimate Y and why? (10 points)\


## Solutions

a) 

```{r}
# read in data
cdi <- read.csv("/cloud/project/HW1/CDI Data.csv")
head(cdi)
colnames(cdi)

# fit regression separately for each variable specified in the question
reg_q2_tot_pop <- lm(Number.of.active.physicians~Total.population, data = cdi)
reg_q2_tot_pop$coefficients

reg_q2_num_beds <- lm(Number.of.active.physicians~Number.of.hospital.beds, data = cdi)
reg_q2_num_beds$coefficients

reg_q2_tot_income <- lm(Number.of.active.physicians~Total.personal.income, data = cdi)
reg_q2_tot_income$coefficients
```
The estimated regression functions are as below:

Number.of.active.physicians = -1.106348e+02 + 2.795425e-03 * Total.population

Number.of.active.physicians = -95.9321847 + 0.7431164 * Number.of.hospital.beds

Number.of.active.physicians = -48.3948489 + 0.1317012 * Total.personal.income
b) 
```{r}
plot(cdi$Total.population, cdi$Number.of.active.physicians, xlab = 'Total Population', ylab = 'Number of Active Physicians')
abline(reg_q2_tot_pop, col = 'red')

plot(cdi$Number.of.hospital.beds, cdi$Number.of.active.physicians, xlab = 'Number of Hospital beds', ylab = 'Number of Active Physicians')
abline(reg_q2_num_beds, col = 'red')

plot(cdi$Total.personal.income, cdi$Number.of.active.physicians, xlab = 'Total Personal Income', ylab = 'Number of Active Physicians')
abline(reg_q2_tot_income, col = 'red')

```
From a visual inspection of the graphs the regression lines do appear to be a good fit for each of the predictor variables; though in all cases the data points are clustered towards the start of the x-axis. 

Hence, some form of outlier removal and/or log transformation might be appropriate here.

c) 
```{r}
sse_q2_num_beds = sum((reg_q2_num_beds$residuals)^2)
mse_q2_num_beds = sse_q2_num_beds/(length(reg_q2_num_beds$residuals)-2)
mse_q2_num_beds

sse_q2_tot_income = sum((reg_q2_tot_income$residuals)^2)
mse_q2_tot_income = sse_q2_tot_income/(length(reg_q2_tot_income$residuals)-2)
mse_q2_tot_income

sse_q2_tot_pop = sum((reg_q2_tot_pop$residuals)^2)
mse_q2_tot_pop = sse_q2_tot_pop/(length(reg_q2_tot_pop$residuals)-2)
mse_q2_tot_pop
```
Number of beds has the lowest mean square error. I would use this variable since of the 3 given to us this has the lowest overall deviation from given values. 

## Problem 4
Refer to the CDI data set. Use the number of active physicians as Y and total personal income as X. Select 1,000 random samples of 400 observations, fit the regression model and record $\beta_{0}$ and $\beta_{1}$ for each selected sample. Calculate the mean and variance of $\beta_{0}$ and $\beta_{1}$ based on the 1,000 different regression line and compare against the regression model in question 3 part a. (20 points)

## Solution: 

```{r}
# create new DF with columns of interest
colnames(cdi) # to get index numbers of columns to copy into new DF
df_q4 <- cdi[,c(8, 16)]
head(df_q4)

# specify parameters and blank vectors to store output
set.seed(123)
n_obs <- 400
n_samples <- 1000
beta0_q4 <- vector(mode = 'integer', length = n_samples)
beta1_q4 <- vector(mode = 'integer', length = n_samples)

for (i in 1:n_samples) {
  # print(i)
  q4_index <- sample(1:nrow(df_q4), n_obs)
  q4_sample_df <- df_q4[q4_index,]
  q4_sample_reg <- lm(Number.of.active.physicians~Total.personal.income, data = q4_sample_df)
  beta0_q4[i] <- q4_sample_reg$coefficients[1]
  beta1_q4[i] <- q4_sample_reg$coefficients[2]
}

q4_beta0_mean <- mean(beta0_q4)
print(q4_beta0_mean)
q4_beta0_var <- var(beta0_q4)
print(q4_beta0_var)

q4_beta1_mean <- mean(beta1_q4)
print(q4_beta1_mean)
q4_beta1_var <- var(beta1_q4)
print(q4_beta1_var)

```
The regression coefficient estimates identified in Q4 are fairly similar to those identified in Q3.a. save for minor differences in 1000th of a decimal point.

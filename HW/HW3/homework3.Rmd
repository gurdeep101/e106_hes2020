---
title: 'CSCI E-106:Assignment 3'
auhtor: 'Gurdeep Singh'
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Submitted by : Gurdeep Singh
### Due Date: September 28, 2020 at 7:20 pm EST
### Instructions

Students should submit their reports on Canvas. The report needs to clearly state what question is being solved, step-by-step walk-through solutions, and final answers clearly indicated. Please solve by hand where appropriate.

Please submit two files: (1) a R Markdown file (.Rmd extension) and (2) a PDF document, word, or html generated using knitr for the .Rmd file submitted in (1) where appropriate. Please, use RStudio Cloud for your solutions.

----------------------------------------------------

## Problem 1

Five observations on Y are to be taken when X = 4, 8, 12, 16, and 20, respectively. The true
regression function is E(Y} = 20 + 4X, and the $\epsilon_i$ are independent N(O, 25). (40 points, 10 points each)

a-) Generate five normal random numbers, with mean 0 and variance 25. Consider these random numbers as the error terms for the five Y observations at X = 4,8, 12, 16, and 20 and calculate $Y_1$, $Y_2$, $Y_3$, $Y_4$ , and $Y_5$. Obtain the least squares estimates $b_0$ and $b_1$, when fitting a straight line to the five cases. Also calculate $Y_h$ when $X_h$ = 10 and obtain a 95 percent confidence interval for E($Y_h$) when $X_h$ = 10. 

```{r}
library(knitr)

rm(list = ls())

set.seed(1023)
x_i <- c(4,8,12,16,20)
err_i <- rnorm(5,0,25) # 5 normal random numbers
y_i_est <- 20 + (4*x_i) + err_i # calculate y_i
err_i; y_i_est # print results

# fit regression and get summary
reg_q1a <- lm(y_i_est~x_i)
summary(reg_q1a)

# calculate E(Y_h) and 95% CI when X_h = 10
x_h <- 10
#y_h <- 20 + (4*xh)
e_y_h <- predict(reg_q1a, data.frame(x_i=x_h), interval = 'confidence', level = 0.95, se.fit = TRUE)
e_y_h

```
Least square estimates are $\beta_O$ = 31.926 and $\beta_1$ = 3.354
${Y_h}$ = 65.46 and 95% CI are 18.70604 and 112.2254

b-) Repeat part (a) 200 times, generating new random numbers each time.

```{r}
n_iter <- 200
b0_1b <- vector(mode = 'integer', length = n_iter)
b1_1b <- vector(mode = 'integer', length = n_iter)
yh_1b <- vector(mode = 'integer', length = n_iter)
yh_lwr_1b <- vector(mode = 'integer', length = n_iter)
yh_upr_1b <- vector(mode = 'integer', length = n_iter)

for (i in 1:n_iter) {
  #print(i)
  err_x <- rnorm(length(x_i), 0, 25)
  y_i_iter <- 20 + (4*x_i) + err_x
  reg_iter <- lm(y_i_iter~x_i)
  b0_1b[i] <- reg_iter$coefficients[1]
  b1_1b[i] <- reg_iter$coefficients[2]
  yh_1b_pred <- predict(reg_iter, data.frame(x_i = x_h), level = 0.95, interval = 'confidence')
  yh_1b[i] <- yh_1b_pred[1]
  yh_lwr_1b[i] <- yh_1b_pred[2]
  yh_upr_1b[i] <- yh_1b_pred[3]
}

# printing 1st 50 values as per piazza discussion
print(yh_1b[1:50])
print(yh_lwr_1b[1:50])
print(yh_upr_1b[1:50])
```

c-) Make a frequency distribution of the 200 estimates $b_1$. Calculate the mean and standard deviation of the 200 estimates $b_1$. Are the results consistent with theoretical expectations?

```{r}
# calculate mean of distribution from 1b above
b1_mean_1c <- mean(b1_1b)
b1_sd_1c <- sd(b1_1b)
b1_mean_1c; b1_sd_1c

# calculate theoretical value of b1
xi_xbar <- x_i - mean(x_i)
yi_ybar <- y_i_est - mean(y_i_est)
b1_th_nr_1c <- sum(xi_xbar * yi_ybar)
b1_th_dr_1c <- sum((xi_xbar)^2)
b1_th_1c <- b1_th_nr_1c/b1_th_dr_1c
b1_th_1c

# calculate theoretical value of sd(b1)
s2_b1_th_nr <- sum((y_i_est - reg_q1a$fitted.values)^2)/(length(x_i)-2)
s2_b1_th_dr <- sum((xi_xbar)^2)
s2_b1_th <- s2_b1_th_nr/s2_b1_th_dr
s2_b1_th

sd_b1_th_1c <- sqrt(s2_b1_th)
sd_b1_th_1c

# Plot the data
par(mfrow = c(1,3))
hist(b0_1b)
hist(b1_1b)
hist(yh_1b)
```

Theoretical mean = 3.353978; calculated mean = 3.768963
Theoretical sd = 2.448833; calculated sd = 2.011892

The values from the mean and SD of the distributions in 1(b) vary by 12% - 20% from the theoretical values above.

This variation is acceptable given that we have calculated mean values using only 200 repetitions. With higher repetition the varation between calculated and theoretical values is likely to be reduced. 

d-) What proportion of the 200 confidence intervals for E($Y_h$) when $X_h$ = 10 include E($Y_h$)? Is this result consistent with theoretical expectations?

```{r}
yh_confint_1d <- data.frame(yh_lwr_1b, yh_upr_1b)
head(yh_confint_1d)

yh_incl_eh <- subset(yh_confint_1d, yh_lwr_1b < 60 & yh_upr_1b > 60)
(dim(yh_incl_eh)[1]/n_iter)*100 # percent of iterations that have E(Y_h)
```

Since 92% of the observations fall within the confidence interval these results are not consistent with theoretical expectations of 95% CI



## Problem 2

Refer to the CDI data set (used in homework 1). The number of active physicians in a CDI (Y) is expected to be related to total population, number of hospital beds, and total personal income. Using $R^2$ as the criterion, which predictor variable accounts for the largest reduction in the variability in the number of active physicians? (20 Point)

```{r}
cdi <- read.csv("CDI Data.csv")
head(cdi)
  
reg_q2_tot_pop <- lm(Number.of.active.physicians~Total.population, data = cdi)
summary(reg_q2_tot_pop)
  
reg_q2_num_beds <- lm(Number.of.active.physicians~Number.of.hospital.beds, data = cdi)
summary(reg_q2_num_beds)
  
reg_q2_tot_income <- lm(Number.of.active.physicians~Total.personal.income, data = cdi)
summary(reg_q2_tot_income)
```

Number of hospital beds has a $R^2$ of 0.9034 and hence accounts for the largest reduction in variability in the number of active physicians.

## Problem 3
Refer to the CDI data set (use in previous homework). For each geographic region, regress per capita income in a CDI (Y) against the percentage of individuals in a county having at least a bachelor's degree (X). Obtain a separate interval estimate of $\beta_1$, for each region. Use a 90 percent confidence coefficient in each case. Do the regression lines for the different regions appear to have similar slopes? (20 points)

```{r}
table(cdi$Geographic.region) # shows 4 geographies with count of 103, 108, 152 and 77 

geo1 <- subset(cdi, Geographic.region == 1, select = c(Per.capita.income, Percent.bachelor.s.degrees))
# visually verified 103 obs with output of table function
geo1_p3_reg <- lm(Per.capita.income~Percent.bachelor.s.degrees, data = geo1)
confint(geo1_p3_reg, level = 0.9)
geo1_p3_reg$coefficients[2]

geo2 <- subset(cdi, Geographic.region == 2, select = c(Per.capita.income, Percent.bachelor.s.degrees))
# visually verified 108 obs with output of table function
geo2_p3_reg <- lm(Per.capita.income~Percent.bachelor.s.degrees, data = geo2)
confint(geo2_p3_reg, level = 0.9)
geo2_p3_reg$coefficients[2]

geo3 <- subset(cdi, Geographic.region == 3, select = c(Per.capita.income, Percent.bachelor.s.degrees))
# visually verified 152 obs with output of table function
geo3_p3_reg <- lm(Per.capita.income~Percent.bachelor.s.degrees, data = geo3)
confint(geo3_p3_reg, level = 0.9)
geo3_p3_reg$coefficients[2]

geo4 <- subset(cdi, Geographic.region == 4, select = c(Per.capita.income, Percent.bachelor.s.degrees))
# visually verified 77 obs with output of table function
geo4_p3_reg <- lm(Per.capita.income~Percent.bachelor.s.degrees, data = geo4)
confint(geo4_p3_reg, level = 0.9)
geo4_p3_reg$coefficients[2]

```

The slopes of the regression lines are as below:

Geo 1 - 552.1588
Geo 2 - 238.6694
Geo 3 - 330.6117
Geo 4 - 440.3157

As seen above the regression lines have different slopes with largest value being more than double of smallest value

## Problem 4
In a small-scale regression study, five observatiol)s on Y were obtained corresponding to X = 1, 4,10, ll, and 14. Assume that $\sigma$ = .6, $\beta_0$ = 5, and $\beta_1$, = 3. (20 points, 10 points each)

a-) What are the expected values MSR and MSE?

```{r}
x_4a <- c(1,4,10,11,14)
sigma <- 0.6
beta0_4a <- 5
beta1_4a <- 3

exp_mse_4a <- sigma^2
exp_mse_4a

exp_msr_4a <- exp_mse_4a + (beta1_4a)*sum((x_4a - mean(x_4a))^2)
exp_msr_4a
```

b-) For determining whether or not a regression relation exists, would it have been better or worse to have made the five observations at X = 6,7, 8, 9, and 1O? Why? Would the same answer apply if the principal purpose were to estimate the mean response for X = 8? Discuss.

```{r}
x_4b <- c(6,7,8,9,10)
xh_4b <- 8

exp_msr_4b <- exp_mse_4a + (beta1_4a)*sum((x_4b - mean(x_4b))^2)
exp_msr_4b

```

The numbers in 4a are better suited to determine whether a regression relation exists since they have a higher MSR. This means that a higher amount of variation is explained by the regression.